{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sbddNKfu14bM"
   },
   "source": [
    "# Project 1 - Information measures\n",
    "\n",
    "The goal of this first project is to get accustomed to the information and uncertainty measures. We ask you to write a brief report (pdf format) collecting your answers to the different questions. All codes must be written in Python inside this Jupyter Notebook. No other code file will be accepted. Note that you can not change the content of locked cells or import any extra Python library than the ones already imported (numpy and pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IPhOrEZC14bP"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "In this project, you will need to use information measures to answer several questions. Therefore, in this first part, you are asked to write several functions that implement some of the main measures seen in the first theoretical lectures. Remember that you need to fill in this Jupyter Notebook to answer these questions. Pay particular attention to the required output format of each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1ZGhdEwn14bP"
   },
   "outputs": [],
   "source": [
    "# [Locked Cell] You can not import any extra Python library in this Notebook.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JFiTJaFG14bQ"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Write a function entropy that computes the entropy $\\mathcal{H(X)}$ of a random variable $\\mathcal{X}$ from its probability distribution $P_\\mathcal{X} = (p_1, p_2, . . . , p_n)$. Give the mathematical formula that you are using and explain the key parts of your implementation. Intuitively, what is measured by the entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {
    "id": "_4WYMYKx14bR"
   },
   "outputs": [],
   "source": [
    "def entropy(Px):\n",
    "    \"\"\"\n",
    "    Computes the entropy from the marginal probability distribution. \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Px :  Marginal probability distribution of the random \n",
    "            variable X in a numpy array where Px[i]=P(X=i)\n",
    "    Return:\n",
    "    -------\n",
    "    - The entropy of X (H(X)) as a number (integer, float or double).\n",
    "    \"\"\"\n",
    "    \n",
    "    H = 0\n",
    "    for px in Px:\n",
    "        if px > 0:\n",
    "            H -= px * np.log2(px)\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LJ2Q0y5a14bR"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Write a function joint_entropy that computes the joint entropy $\\mathcal{H(X,Y)}$ of two discrete random variables $\\mathcal{X}$ and $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$. Give the mathematical formula that you are using and explain the key parts of your implementation. Compare the entropy and joint_entropy functions (and their corresponding formulas), what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {
    "id": "qqK4N-_q14bS"
   },
   "outputs": [],
   "source": [
    "def joint_entropy(Pxy):\n",
    "    \"\"\"\n",
    "    Computes the joint entropy from the joint probability distribution.  \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxy:  joint probability distribution of X and Y \n",
    "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
    "    Return:\n",
    "    -------\n",
    "    - The joint entropy H(X,Y) as a number (integer, float or double).\n",
    "    \"\"\"\n",
    "    \n",
    "    H = 0\n",
    "    for i in range(Pxy.shape[0]):\n",
    "        for j in range(Pxy.shape[1]):\n",
    "            px_y = Pxy[i][j]\n",
    "            if px_y > 0:\n",
    "                H -= px_y * np.log2(px_y)\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gxqVI5qt14bS"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Write a function conditional_entropy that computes the conditional entropy $\\mathcal{H(X|Y)}$ of a discrete random variable $\\mathcal{X}$ given another discrete random variable $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$. Give the mathematical formula that you are using and explain the key parts of your implementation. Describe an equivalent way of computing that quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {
    "id": "P-HBnYxh14bS"
   },
   "outputs": [],
   "source": [
    "def conditional_entropy(Pxy):\n",
    "    \"\"\"\n",
    "    Computes the conditional entropy from the joint probability distribution.\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxy:  joint probability distribution of X and Y \n",
    "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
    "    Return:\n",
    "    -------\n",
    "    - The conditional entropy H(X|Y) as a number (integer, float or double)\n",
    "    \"\"\"\n",
    "    \n",
    "    Hy = entropy(np.sum(Pxy, axis=0))\n",
    "    Hxy = joint_entropy(Pxy)\n",
    "    \n",
    "    return Hxy - Hy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t5a3I5RV14bT"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Write a function mutual_information that computes the mutual information $\\mathcal{I(X;Y)}$ between two discrete random variables $\\mathcal{X}$ and $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$ . Give the mathematical formula that you are using and explain the key parts of your implementation. What can you deduce from the mutual information $\\mathcal{I(X;Y)}$ on the relationship between $\\mathcal{X}$ and $\\mathcal{Y}$? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {
    "id": "1-97kFCi14bT"
   },
   "outputs": [],
   "source": [
    "def mutual_information(Pxy):\n",
    "    \"\"\"\n",
    "    Computes the mutual information I(X;Y) from joint probability distribution\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxy:  joint probability distribution of X and Y \n",
    "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
    "    Return:\n",
    "    -------\n",
    "    - The mutual information I(X;Y) as a number (integer, float or double)\n",
    "    \"\"\"\n",
    "    \n",
    "    Px = np.sum(Pxy, axis=1)\n",
    "    Py = np.sum(Pxy, axis=0)\n",
    "    Hx = entropy(Px)\n",
    "    Hy = entropy(Py)\n",
    "    Hxy = joint_entropy(Pxy)\n",
    "    \n",
    "    return Hx + Hy - Hxy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HLC7Qkyk14bU"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Let $\\mathcal{X}$, $\\mathcal{Y}$ and $\\mathcal{Z}$ be three discrete random variables. Write the functions cond_joint_entropy and cond_mutual_information that respectively compute $\\mathcal{H(X,Y|Z)}$ and $\\mathcal{I(X;Y|Z)}$ of two discrete random variable $\\mathcal{X}$, $\\mathcal{Y}$ given another discrete random variable $\\mathcal{Z}$ from their joint probability distribution $P_\\mathcal{X,Y,Z}$. Give the mathematical formulas that you are using and explain the key parts of your implementation.\n",
    "Suggestion: Observe the mathematical definitions of these quantities and think how you could derive them from the joint entropy and the mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {
    "id": "IeWejuNa14bU"
   },
   "outputs": [],
   "source": [
    "def cond_joint_entropy(Pxyz):\n",
    "    \"\"\"\n",
    "    Computes the conditional joint entropy of X, Y knowing Z \n",
    "    from the joint probability distribution Pxyz\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxyz: joint probability distribution of X, Y and Z\n",
    "            in a 3-D array where Pxyz[i][j][k]=P(X=i,Y=j,Z=k)\n",
    "    Return:\n",
    "    -------\n",
    "    - The conditional joint entropy H(X,Y|Z) as a number (integer, float or double)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute Pxz\n",
    "    Pxz = np.sum(Pxyz, axis=1)\n",
    "\n",
    "    # Compute Hx_z\n",
    "    Hx_z = conditional_entropy(Pxz)\n",
    "    \n",
    "    Hy_xz = 0\n",
    "    \n",
    "    # Compute Hy_xz\n",
    "    for i in range(Pxyz.shape[0]):\n",
    "        for k in range (Pxyz.shape[2]):\n",
    "            for j in range(Pxyz.shape[1]):\n",
    "                if(Pxyz[i][j][k] != 0):\n",
    "                    Hy_xz -= Pxyz[i][j][k]*np.log2(Pxyz[i][j][k]/np.sum(Pxyz, axis = 1)[j][k])\n",
    "    \n",
    "    Hxy_z = Hx_z + Hy_xz\n",
    "    \n",
    "    return Hxy_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_mutual_information(Pxyz):\n",
    "    \"\"\"\n",
    "    Computes the conditional mutual information of X, Y knowing Z \n",
    "    from joint probability distribution Pxyz\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxyz: joint probability distribution of X, Y and Z\n",
    "            in a 3-D array where Pxyz[i][j][k]=P(X=i,Y=j,Z=k)\n",
    "    Return:\n",
    "    -------\n",
    "    - I(X;Y|Z): The conditional joint entropy as a number (integer, float or double)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute Pxz\n",
    "    Pxz = np.sum(Pxyz, axis=1)\n",
    "\n",
    "    # Compute Hx_z\n",
    "    Hx_z = conditional_entropy(Pxz)\n",
    "    \n",
    "    Hx_yz = 0\n",
    "    \n",
    "    # Compute Hx_yz\n",
    "    for i in range(Pxyz.shape[0]):\n",
    "        for k in range (Pxyz.shape[2]):\n",
    "            for j in range(Pxyz.shape[1]):\n",
    "                if(Pxyz[i][j][k] != 0):\n",
    "                    Hx_yz -= Pxyz[i][j][k]*np.log2(Pxyz[i][j][k]/np.sum(Pxyz, axis = 0)[j][k])\n",
    "    \n",
    "    Ixy_z = Hx_z - Hx_yz\n",
    "    \n",
    "    return Ixy_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5TfG2aWt14bV"
   },
   "outputs": [],
   "source": [
    "# [Locked Cell] Evaluation of your functions by the examiner. \n",
    "# You don't have access to the evaluation, this will be done by the examiner.\n",
    "# Therefore, this cell will return nothing for the students.\n",
    "import os\n",
    "if os.path.isfile(\"private_evaluation.py\"):\n",
    "    from private_evaluation import unit_tests\n",
    "    unit_tests(entropy, joint_entropy, conditional_entropy, mutual_information, cond_joint_entropy, cond_mutual_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4MSJsvry14bV"
   },
   "source": [
    "### Football outcome\n",
    "\n",
    "You may create cells below to answer the different questions related to football outcome. Unlike in the first part (Implementation), you are free to define as many cells as you need below to answer the different questions. Try to be structured and clear in your code (comment it if necessary). Note that you have to answer the questions in the pdf report, including the numbers you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data into a pandas dataframe\n",
    "data = pd.read_csv(\"data.csv\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "Compute and report the entropy of each variable, and compare each value with its corresponding variable cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {
    "id": "nck547oL14bW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome : entropy =  [1.33488]\n",
      "outcome : cardinality =  [3.]\n",
      "previous_outcome : entropy =  [1.483]\n",
      "previous_outcome : cardinality =  [3.]\n",
      "day : entropy =  [2.80656]\n",
      "day : cardinality =  [7.]\n",
      "time : entropy =  [0.93252]\n",
      "time : cardinality =  [3.]\n",
      "month : entropy =  [3.58263]\n",
      "month : cardinality =  [12.]\n",
      "wind_speed : entropy =  [1.58471]\n",
      "wind_speed : cardinality =  [3.]\n",
      "weather : entropy =  [1.76408]\n",
      "weather : cardinality =  [4.]\n",
      "location : entropy =  [0.99994]\n",
      "location : cardinality =  [2.]\n",
      "capacity : entropy =  [1.53391]\n",
      "capacity : cardinality =  [4.]\n",
      "stadium_state : entropy =  [0.63955]\n",
      "stadium_state : cardinality =  [2.]\n",
      "injury : entropy =  [0.99984]\n",
      "injury : cardinality =  [2.]\n",
      "match_type : entropy =  [0.9999]\n",
      "match_type : cardinality =  [2.]\n",
      "opponent_strength : entropy =  [1.58435]\n",
      "opponent_strength : cardinality =  [3.]\n"
     ]
    }
   ],
   "source": [
    "name_column = data.columns.tolist()\n",
    "entropies = np.zeros((13,1))\n",
    "cardinality = np.zeros((13,1))\n",
    "\n",
    "for i in range(0,13):\n",
    "    proba_distri = np.zeros((data[name_column[i]].value_counts().shape[0],1))\n",
    "    \n",
    "    # Computing probability distribution for each variable \n",
    "    for j in range(0,data[name_column[i]].value_counts().shape[0]):\n",
    "        proba_distri[j] = data[name_column[i]].value_counts()[j]/data.shape[0]\n",
    "        \n",
    "    # Computing entropy for each variable    \n",
    "    entropies[i] = entropy(proba_distri) \n",
    "    \n",
    "    # Computing cardinality for each variable \n",
    "    cardinality[i] = data[name_column[i]].value_counts().shape[0]\n",
    "    \n",
    "    print(name_column[i], \": entropy = \", np.round(entropies[i], 5))\n",
    "    print(name_column[i], \": cardinality = \", cardinality[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "\n",
    "Compute and report the conditional entropy of outcome given each of the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of outcome :  [1.33487925] \n",
      "\n",
      "Conditional entropy of outcome given each of the other variables:\n",
      "\n",
      "previous_outcome :  1.18148\n",
      "day :  1.33349\n",
      "time :  1.3338\n",
      "month :  1.33036\n",
      "wind_speed :  1.33473\n",
      "weather :  1.33384\n",
      "location :  1.33351\n",
      "capacity :  1.33202\n",
      "stadium_state :  1.33432\n",
      "injury :  1.33024\n",
      "match_type :  1.33483\n",
      "opponent_strength :  0.93861\n"
     ]
    }
   ],
   "source": [
    "# Calculate probability distribution of outcome\n",
    "proba_distri = np.zeros((data['outcome'].value_counts().shape[0],1))  \n",
    "for j in range(0,data['outcome'].value_counts().shape[0]):\n",
    "    proba_distri[j] = data['outcome'].value_counts()[j]/data.shape[0] \n",
    "\n",
    "# Calculate entropy of outcome\n",
    "outcome_entropy = entropy(np.round(proba_distri, 5))\n",
    "print(\"Entropy of outcome\", \": \", outcome_entropy, \"\\n\")\n",
    "\n",
    "cond_entropy = {}\n",
    "for col in data.columns:\n",
    "    if col != 'outcome':\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(data['outcome'], data[col], normalize=True)\n",
    "        \n",
    "        # Convert into a numpy array\n",
    "        Pxy = np.array(contingency_table)\n",
    "        \n",
    "        # Calculate conditional entropy\n",
    "        cond_entropy[col] = conditional_entropy(Pxy)\n",
    "\n",
    "print(\"Conditional entropy of outcome given each of the other variables:\\n\")\n",
    "for col, ce in cond_entropy.items():\n",
    "    print(col, \": \", np.round(ce, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "\n",
    "Compute the mutual information between the variables month and capacity. What about the variables day and time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "month and capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information between month and capacity: 0.00607\n"
     ]
    }
   ],
   "source": [
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(data['month'], data['capacity'], normalize=True)\n",
    "\n",
    "# Convert into numpy array\n",
    "Pxy = np.array(contingency_table)\n",
    "\n",
    "# Calculate mutual information\n",
    "mutual_info = mutual_information(Pxy)\n",
    "\n",
    "print(\"Mutual information between month and capacity:\", np.round(mutual_info, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "day and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information between day and time: 0.50461\n"
     ]
    }
   ],
   "source": [
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(data['day'], data['time'], normalize=True)\n",
    "\n",
    "# Convert into a numpy array\n",
    "Pxy = np.array(contingency_table)\n",
    "\n",
    "# Calculate mutual information\n",
    "mutual_info = mutual_information(Pxy)\n",
    "\n",
    "print(\"Mutual information between day and time:\", np.round(mutual_info, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "\n",
    "Mutual Information between outcome and each of the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between outcome and each of the other variables:\n",
      "previous_outcome :  0.1534\n",
      "day :  0.00139\n",
      "time :  0.00108\n",
      "month :  0.00452\n",
      "wind_speed :  0.00015\n",
      "weather :  0.00104\n",
      "location :  0.00137\n",
      "capacity :  0.00286\n",
      "stadium_state :  0.00055\n",
      "injury :  0.00464\n",
      "match_type :  5e-05\n",
      "opponent_strength :  0.39627\n"
     ]
    }
   ],
   "source": [
    "mutual_info = {}\n",
    "for col in data.columns:\n",
    "    if col != 'outcome':\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(data['outcome'], data[col], normalize=True)\n",
    "\n",
    "        # Convert into a numpy array\n",
    "        Pxy = np.array(contingency_table)\n",
    "        \n",
    "        # Calculate mutual information\n",
    "        mutual_info[col] = mutual_information(Pxy)\n",
    "\n",
    "print(\"Mutual Information between outcome and each of the other variables:\")\n",
    "for col, mi in mutual_info.items():\n",
    "    print(col, \": \", np.round(mi, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information between outcome and each of the other variables given previous_outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between outcome and each of the other variables given previous_outcome:\n",
      "day :  0.00474\n",
      "time :  0.00347\n",
      "month :  0.01369\n",
      "wind_speed :  0.00212\n",
      "weather :  0.00303\n",
      "location :  0.00213\n",
      "capacity :  0.00495\n",
      "stadium_state :  0.00062\n",
      "injury :  0.009\n",
      "match_type :  0.00051\n",
      "opponent_strength :  0.24459\n"
     ]
    }
   ],
   "source": [
    "cond_mutual_info = {}\n",
    "for col in data.columns:\n",
    "    if col != 'outcome' and col != 'previous_outcome':\n",
    "\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(index=[data['outcome'], data[col]], columns = data['previous_outcome'], normalize=True)\n",
    "\n",
    "        # Convert the contingency table to a 3-D array\n",
    "        Pxyz = contingency_table.values.reshape((len(set(data['outcome'])), len(set(data[col])), len(set(data['previous_outcome']))))\n",
    "\n",
    "        # Calculate mutual information\n",
    "        cond_mutual_info[col] = cond_mutual_information(Pxyz)\n",
    "\n",
    "print(\"Mutual Information between outcome and each of the other variables given previous_outcome:\")\n",
    "for col, cmi in cond_mutual_info.items():\n",
    "    print(col, \": \", np.round(cmi, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information between stadium_state and weather when location is home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information between stadium_state and weather when location is home: 0.0\n"
     ]
    }
   ],
   "source": [
    "home_data = data[data['location'] == 'home']\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(home_data['stadium_state'], home_data['weather'], normalize=True)\n",
    "\n",
    "# Convert into numpy array\n",
    "Pxy = np.array(contingency_table)\n",
    "\n",
    "# Calculate mutual information\n",
    "mutual_info = mutual_information(Pxy)\n",
    "\n",
    "print(\"Mutual information between stadium_state and weather when location is home:\", mutual_info)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "977f9d2e455d2b6cd8f4f8ab88366a61d3a4c7ae8f08a6d218b82c9c3e551156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
